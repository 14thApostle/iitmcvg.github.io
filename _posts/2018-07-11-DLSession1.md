---
layout: single
title: "Summer School Deep Learning Session 1"
category: summer_school
description: " Hello "
mathjax: true
---

## Math behind Deep Learning and Introduction to Keras

The session note book can be found [here](https://raw.githubusercontent.com/iitmcvg/Content/master/Sessions/Summer_School_2018/Session_DL_2/Session2.ipynb)

### ML Techniques 

_A Broad division_
* **classification:** predict class from observations 

* **clustering:** group observations into
“meaningful” groups

* **regression (prediction):** predict value from observations

### Some Terminology
* **Features**  
  – The number of features or distinct traits that can be used to describe  each item in a quantitative manner. 
* **Samples**  
  – A sample is an item to process (e.g. classify). It can be a document, a picture, a sound, a video, a row in database or CSV file, or whatever you can describe with a fixed set of quantitative traits.
* **Feature vector**: An n-dimensional vector of numerical features that represent some object.
*  **Feature extraction**  
 – Preparation of feature vector  
 – Transform the data in the high-dimensional space to a space of fewer dimensions.  
* **Training/Evolution set**
  – Set of data to discover potentially predictive relationships.

# Introduction to Keras

Keras is a **high-level** neural network API, written in Python. It was developed with a focus on enabling **fast experimentation**. 

Any program written with Keras has **5 steps**:
- Define the model
- Compile
- Fit (train)
- Evaluate 
- Predict

Here, we will discuss the implementation of a simple classification task using Keras, with a **softmax**  (logistic) classifier.

## Linear Classifier

A linear classifier uses a score function f of the form \begin{equation} f(x_i,W,b)=Wx_i+b \end{equation}  where the matrix **W** is called the **weight matrix** and **b** is called the **bias vector**.

## Softmax Classifier

We have a dataset of points $$(x_i,y_i)$$, where $$x_i$$ is the input and $$y_i$$ is the output class.
Consider a linear classifier for which **class scores** *f* can be found as $$f(x_i;W) = Wx_i$$. For this multi-class classification problem, let us denote the score for the **class j** as $$s_j = f(x_i;W)_j$$, in short as $$f_j$$.

Let $$z = Wx$$. Then the scores for class j will be computed as:
\begin{equation}
s_j(z) = \frac{e^{z_j}}{\Sigma{_k}{e^{z_k}}}
\end{equation}
 This is known as the **softmax function**.



If you’ve heard of the **binary Logistic Regression** classifier before, the Softmax classifier is its generalization to **multiple classes**. The Softmax classifier gives an intuitive output (normalized class probabilities) and also has a probabilistic interpretation that we will describe shortly. In the Softmax classifier, we interpret the scores as the unnormalized log probabilities for each class and have a **cross-entropy loss** that has the form:

\begin{equation}
L_i = -log(\frac{e^{f_{y_i}}}{\Sigma{_j}{e^{f_j}}})
\end{equation}

### Example

Let us consider a dataset with 4 input features and 3 output classes. So, the shape of the **weight matrix** (W) is 3x4 and that of the **input vector** $$(x_i)$$ is 4x1. Therefore, we get an output of shape 3x1, which is given by $$Wx+b$$. Also, for this particular $$x_i$$, the **output class** $$(y_i)$$ is 2 $$(3^{rd} class)$$. 

   ![image](https://i.imgur.com/JU7OQV7.png){: .align-center}

Now we have the **unnormalized** class scores. Now we will take the softmax function of these class scores. Finally, we can observe that the normalized class scores sum to 1. The loss function is given by $$-log(s_2(z)) = -log(0.353) = 1.04$$.

   ![image](https://i.imgur.com/XYw86Hq.png){: .align-center}

 ```
import numpy as np
import tensorflow as tf
import keras
from sklearn.datasets import make_blobs
from keras.models import Sequential
from keras.layers import Dense, Activation
from keras.regularizers import L1L2
import pandas as pd
from sklearn.model_selection import train_test_split
from keras.utils import np_utils 
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
```
Now that you have understood the concept behind softmax classifier, let's jump into the code.

Let us create our dataset using **scikit-learn's** make_blobs function. The number of points in our dataset is 4000 and we have 4 classes as shown below. The dimension of the data (no.of features) is chosen to be 2, so that it is easy to plot and visualize. In reality, image classification tasks will have a very large number of dimensions.

```
data = make_blobs(n_samples=4000, n_features=2, centers=4, cluster_std=2,random_state=101)
plt.scatter(data[0][:,0],data[0][:,1],c=data[1],cmap='rainbow')
plt.show()
```
![alt](/assets/images/posts/Summer_School/DL1/im1.png){: .align-center}

We will use scikit-learn's **train_test_split** function to split our dataset into train and test, with a ratio of 4:1. After this, we will convert our labels (y) into **one-hot** vectors before passing it into our classifier model.

```
X = data[0]
y = data[1]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
Y_train = np_utils.to_categorical(y_train, 4) 
Y_test = np_utils.to_categorical(y_test, 4)
print("y_train labels: ",y_train)
print("Y_train one-hot labels: \n",Y_train)
```

y_train labels:  [1 3 3 ... 0 1 2]
Y_train one-hot labels: 
 [[0. 1. 0. 0.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 ...
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]]

This is where we will **define our model**. 

The **Sequential** model is a linear stack of layers to which layers can be added using the **add()** function. 

The **Dense** function here, takes 3 parameters - **no.of output classes, input dimension, type of actvation**. 

**Epoch**: One pass of the entire set of training examples.

**Batch size**: Number of training examples used for one pass (iteration).

```
# Define the model
model = Sequential() 
model.add(Dense(4, input_dim=2, activation='softmax')) 
batch_size = 4
nb_epoch = 10
```

The **compile** function configures the model for training with the appropriate *optimizer* and *loss function*. Here we have selected categorical cross-entropy loss since it is a multi-class classification problem.

The **fit** function trains the model for the specified number of epochs and batch size. 

The **evaluate** computes the accuracy on the test set after training.

```
model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) 
history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(X_test, Y_test)) 
score = model.evaluate(X_test, Y_test, verbose=0) 
print('Test score:', score[0]) 
print('Test accuracy:', score[1])
```
Train on 3200 samples, validate on 800 samples

After 10 epochs:

Test score: 0.18148446649312974

Test accuracy: 0.9225

Now, let us generate the predictions. 

The **predict** function returns a 4-element vector of softmax probabilities for each class. The correct prediction is obtained using numpy's **argmax()** function, which finds the index of the maximum element in the vector.

```
Y_pred = model.predict(X_test)
print("Softmax predictions: \n",Y_pred)
y_pred = np.argmax(Y_pred, axis=1)
print("\nClass with maximum probability: \n",y_pred)
```

Softmax predictions: 

 [[1.3248611e-03 2.5521611e-09 9.9851722e-01 1.5793661e-04]

 [6.4598355e-03 9.7919554e-05 5.8634079e-04 9.9285591e-01]
 ...

 ...


 [1.2989193e-03 6.6605605e-09 9.8468834e-01 1.4012725e-02]

 [3.0959516e-03 9.9689341e-01 8.6714969e-11 1.0548322e-05]]


Class with maximum probability: 

 [2 3 0 3 2 2 2 2 3 0 2 3 0 2 3 0 2 2 1 1 1 2 1 2 2 2 1 2 2 1 2 1 0 1 3 3 1

 0 1 3 1 0 1 1 0 0 3 0 1 1 0 3 2 0 2 2 1 1 1 0 3 0 3 1 0 1 3 2 1 0 1 0 3 3

....

....

 1 3 1 0 1 2 2 0 0 3 1 3 1 3 2 1 0 3 2 1 1 1 0 1 2 2 3 2 0 2 3 3 0 1 3 2 3

 2 1 2 2 2 0 0 2 0 1 3 1 0 1 3 3 0 3 0 0 1 2 1]

```
f, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(10,6))
ax1.set_title('Predictions')
ax1.scatter(X_test[:,0],X_test[:,1],c=y_pred,cmap='rainbow')
ax2.set_title("Original")
ax2.scatter(X_test[:,0],X_test[:,1],c=y_test,cmap='rainbow')
```

![alt](/assets/images/posts/Summer_School/DL1/im2.png){: .align-center}

### Visualizing the decision boundary 

Let us create a grid of points 10000 in the 2D space between $$x = \pm20$$ and $$y=\pm20$$. 

```
x = np.linspace(-20, 20, 100)
xx, yy = np.meshgrid(x, x)
data = np.dstack((xx,yy))
data = data.reshape(-1,2)
data.shape
```

(10000, 2)

Now, we will obtain the predictions from the model.

```
y = model.predict(data)
y_class = np.argmax(y, axis=1)
plt.scatter(data[:,0], data[:,1], c=y_class, cmap='rainbow')
plt.title("Decision Boundary Predicted by Softmax Classifier")
plt.show()
```

![alt](/assets/images/posts/Summer_School/DL1/im3.png){: .align-center}

## Understanding Linear classifiers

Now, let us make another dataset with make_blobs. But, this will have 2 clusters. 

```
data = make_blobs(n_samples=4000, n_features=2, centers=2, cluster_std=2.5,random_state=101)
plt.scatter(data[0][:,0],data[0][:,1],c=data[1],cmap='rainbow')
plt.show()
```

![alt](/assets/images/posts/Summer_School/DL1/im4.png){: .align-center}

Convert to one-hot labels.

```
X = data[0]
y = data[1]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
Y_train = np_utils.to_categorical(y_train, 2) 
Y_test = np_utils.to_categorical(y_test, 2)
print("y_train labels: ",y_train.shape)
print("Y_train one-hot labels: \n",Y_train.shape)
```

Define the model and train it.

```
model = Sequential() 
model.add(Dense(2, input_dim=2, name='wx_b'))
model.add(Activation('softmax', name='softmax'))
batch_size = 128
nb_epoch = 100
model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) 
history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(X_test, Y_test)) 
score = model.evaluate(X_test, Y_test, verbose=0) 
print('Test score:', score[0])
print('Test accuracy:', score[1])
```

After 100 epochs:

Test score: 0.05136142261326313

Test accuracy: 0.99

### What has happened to our data as it `propogates forward` through this classifier

Retrieve the weights and biases from the learned model.

```
w_dense = (model.layers[0].get_weights()[0])
b_dense =  (model.layers[0].get_weights()[1])
```

Let us visualize the weights and bias terms which have been learned.

```
sns.heatmap(w_dense, linewidth=0.5)
plt.show()
print (w_dense)
```

![alt](/assets/images/posts/Summer_School/DL1/im14.png){: .align-center}

[[ 0.18243906 -0.32536864]

 [ 0.80486184  0.29774043]]

```
sns.heatmap(b_dense.reshape((2,1)))
plt.show()
print (b_dense)
```

![alt](/assets/images/posts/Summer_School/DL1/im15.png){: .align-center}

[ 1.4485976 -1.4485976]

```
plt.scatter(X_test[:,0], X_test[:,1], c = Y_test[:,1], cmap='rainbow')
plt.title('Scatter plot of test data')
plt.show()
```

![alt](/assets/images/posts/Summer_School/DL1/im16.png){: .align-center}

Take dot product of W and *X _test*.

```
wx = X_test.dot(w_dense)
wx_b = wx + b_dense
plt.scatter(wx_b[:,0], wx_b[:,1], c=Y_test[:,1], cmap='rainbow')
plt.scatter(wx[:,0], wx[:,1], c=Y_test[:,1], cmap='viridis')
plt.quiver([0], [0], b_dense[0], b_dense[1], scale = 21, label = 'Bias Vector',color='w')
plt.legend(loc='best')
plt.xlim([-20, 20])
plt.show()
```

![alt](/assets/images/posts/Summer_School/DL1/im17.png){: .align-center}

### Visualizing the predictions

```
y = model.predict(X_test)
plt.scatter(y[:,0], y[:,1], c=Y_test[:,1], cmap='rainbow')
```

![alt](/assets/images/posts/Summer_School/DL1/im18.png){: .align-center}

## Whats Regression?

## Regression using Neural Nets

```
def create_dataset(f, x=None, std=0.1):
  if x is None:
    x = np.linspace(-20, 20, 1000)
  
  noise = np.random.randn(len(x))*std
  y = f(x)+noise
  return (x,y)

def regress_fn(x):
  return 1/(1 + np.exp(x))

x,y = create_dataset(regress_fn)

plt.scatter(x,y)
plt.show()
```

![alt](/assets/images/posts/Summer_School/DL1/im19.png){: .align-center}

```
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
plt.scatter(X_train, y_train, label='train')
plt.scatter(X_test, y_test, label='test')
plt.legend(loc='best')
plt.show()
```
![alt](/assets/images/posts/Summer_School/DL1/im20.png){: .align-center}

```
model.compile(optimizer='sgd', loss='mean_squared_error') 
history = model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(X_test, y_test)) 
score = model.evaluate(X_test, y_test, verbose=0) 
print('Test score:', score)
```
loss: 0.0117

```
x = np.linspace(-20,20,1000)
y = model.predict(x)
plt.scatter(X_train, y_train, label='Dataset')
plt.scatter(x, y, label='Predicted Function')
plt.legend(loc='best')
plt.show()
```

![alt](/assets/images/posts/Summer_School/DL1/im21.png){: .align-center}

Now that we have a basic understanding of how neural networks work, let's move on to more complicated stuff!!
